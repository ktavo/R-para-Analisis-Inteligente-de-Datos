install.packages(plotrix)
install.packages("plotrix")
install.packages("plotrix")
install.packages("plotrix")
library("plotrix", lib.loc="C:/Program Files/R/R-3.4.0/library")
install.packages("plotrix")
setInternet2(set = TRUE)
setInternet2(use = TRUE)
install.packages("plotrix")
Modelos
Modelos<-2010:2016
Modelos
Ventas<-c(2,4,0,9,3,7,6)
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(7))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(1))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(1))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(10))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(100))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(7))
plot(Modelos,Ventas,type="h",lty="solid", lwd=4, col=heat.colors(9))
hist(PESO, col=maroon1, density=18, border= bluevliolet)
hist(PESO,col="maroon1",breaks=seq(0,85,5),density=18,angle=70)
no.gar=c(258,280)
gar.si=c(184,719)
mat=rbind(no.gar,gar.si)
colnames(mat)=c("no.cons","si.cons")
mosaicplot(mat,col=terrain.colors(2:3),main="Gráfico de mosaicos")
mosaicplot(mat,col=terrain.colors(2:3),main="Gráfico de mosaicos")
install.packages(plotrix)
install.packages("plotrix")
read.csv2("E:/UBA/Análisis Inteligente de Datos/1- 8 de abril - 14 de abril/movies.csv")
moviesDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/1- 8 de abril - 14 de abril/movies.csv")
attach(moviesDB)
frec.tipo <- (table(3))
etiquetas<-c("Comedy"; "Romance"; "Children"; "Adventure")
etiquetas<-c("Comedy", "Romance", "Children", "Adventure")
pie3D(frec.tipo,labels=etiquetas,explode=0.3,labelcex=0.8,radius=1.5)
install.packages("pie3D")
ap <- available.packages()
ap
install.packages("pie3d")
install.packages("pie3D")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.xls")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.xls")
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.csv")
recepcionistasDB
recepcionistasDB
recepcionistasDB <- read.csv2("E:/UBA/Análisis Inteligente de Datos/4- 29 de abril - 5 de mayo/recepcionistas.csv")
recepcionistasDB
View(recepcionistasDB)
total <-rowMeans(recepcionistasDB)
total <-rowMeans(recepcionistasDB, -ncol(1))
total <-rowMeans(recepcionistasDB[, -ncol(1)])
> recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2, 7)), na.rm = TRUE)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2, 7)), na.rm = TRUE)
recepSubset = subset(recepcionistasDB, select = c(2,7))
recepSubset
View(recepSubset)
View(recepSubset)
View(recepSubset)
remove(recepSubset)
View(recepSubset)
View(recepcionistasDB)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2; 7)), na.rm = TRUE)
recepcionistasDB$mean <- rowMeans(subset(recepcionistasDB, select = c(2: 7)), na.rm = TRUE)
View(recepcionistasDB)
recepcionistasDB$juez1 <- rowMeans(subset(recepcionistasDB, select = c(2: 4)), na.rm = TRUE)
recepcionistasDB$juez2 <- rowMeans(subset(recepcionistasDB, select = c(5: 7)), na.rm = TRUE)
View(recepcionistasDB)
recepcionistasDBNormalizada <- subset(recepcionistasDB, select = c(2:7))
recepcionistasDBNormalizada
view(recepcionistasDBNormalizada)
recepcionistasDBNormalizada <- scale(recepcionistasDBNormalizada)
view(recepcionistasDBNormalizada)
view(recepcionistasDB)
View(recepcionistasDBNormalizada)
meantest <- rowMeans(subset(recepcionistasDBNormalizada, select = c(1: 6)), na.rm = TRUE)
meantest
meantest <- rowMeans(recepcionistasDBNormalizada)
View(meantest)
View(recepcionistasDBNormalizada)
remove(meantest)
recepcionistasDBNormalizada <- scale(recepcionistasDBNormalizada, 0 ,1)
recepcionistasDBNormalizada <- rnorm(recepcionistasDB)
View(recepcionistasDBNormalizada)
View(recepcionistasDB)
recepcionistasDBNormalizada <- rnorm(subset(recepcionistasDB, select = c(2:7)))
View(recepcionistasDBNormalizada)
remove(recepcionistasDBNormalizada)
pointsRecepcionistas <- subset(recepcionistasDB, select = c(2:7))
View(pointsRecepcionistas)
normalizedPoints <- rnorm(pointsRecepcionistas)
View(normalizedPoints)
remove(normalizedPoints)
View(recepcionistasDB)
normalizedDB <- apply(recepcionistasDB, [,2], 1 , scale)
normalizedDB <- apply(recepcionistasDB[,2], 1 , scale)
install.packages("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
library("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
View(pointsRecepcionistas)
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to c (-1,1))
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to=c(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, sum)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, (-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, to=c(-1,1))
normalizedRecepcionistasDB
rowMeans <- apply(normalizedRecepcionistasDB, 2 , colMeans)
View(normalizedRecepcionistasDB)
pointsRecepcionistas
install.packages("ca")
library("ca")
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2))
fum
View fum
View(fum)
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2), nrow = 5, ncol = 4, byrow = TRUE)
View(fum)
tabum = addmargins(fum)
View(tabum)
colnames(fum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
colnames(tabum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
rownames(tabum) = c("G.Senior", "G.Junior", "EmpSenior", "EmpJunior", "Secretarira", "Total_Col")
View(tabum)
objeto = ca(tabum, nd=2)
plot(objeto, main="Biplot Simétrico")
Arbequina=c(34.5, 20.1, 21.8 ,18.2 ,19.5 ,20.2,22.5 ,23.9 ,22.1 ,24.2)
Carolea=c (16.4, 14.8, 17.8, 12.3, 11.9, 15.5, 13.4,16 ,15.8 ,16.2)
shapiro.test(Arbequina) # testeamos la normalidad de los datos
shapiro.test(CArolea) # testeamos la normalidad de los datos
shapiro.test(Carolea) # testeamos la normalidad de los datos
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
Wilcoxon bilateral
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
#Los datos no satisfacen el supuesto de normalidad distribucional, luego no puede aplicarse un test t.
library(RVAideMemoire)
install.packages("RVAdeMemoire")
install.packages("RVAideMemoire")
library(RVAideMemoire)
#te.aov<-aov(vitam  marca) # cargamos el análisis de la varianza en el objeto te.aov
#summary(te.aov) # pedimos la síntesis de la prueba
#bartlett.test(vitam,marca)
install.packages("Rcmdr)")
install.packages("Rcmdr")
install.packages("reshape2")
install.packages("car")
install.packages("car")
library("Rcmdr")
install.packages("nortest")
#Chernoff Faces
#Los diversos datos se transforman en caras, sirve para identificar similitudes entre individuos
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
library("tcltk2")
library("aplpack")
library("readxl")
#Importa Archivo para trabajar
galletitas = read_excel("../Data Sets Apunte Teórico/galletitas.xlsx")
attach(galletitas)
#Agrupamos las galletitas saladas
saladas = split(galletitas, galletitas$Tipo)$salada
#Genera las caras de Chernoff
faces(saladas[,2:6], nrow.plot = 2, ncol.plot = 5, face.type = 1, labels = saladas$Marca)
#Correlogram
#el color azul indica correlación positiva, el color rojo indica correlación negativa.
#Cuanto mayor es la intensidad del color más cercano a 1 en el caso positivo y a 􀀀1 en el cas
#o negativo se encuentra la correlación.
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
# Importa la base desde el archivo de excel
IMCInfantil = read_excel("../Data Sets Apunte Teórico/IMCinfantil.xlsx")
#Almacena la base en memoria
attach(IMCInfantil)
#Arma una sub-base con las variables seleccionadas de IMCInfantil
base.niños = data.frame(EDAD, PESO, TALLA, IMC, CC)
#Modifica la variable para que correlacione de forma negativa con las otras
base.niños$CC = max(base.niños$CC) - base.niños$CC
#Calcula la matriz de correlación
M = cor(base.niños)
#Genera el correlograma
corrplot.mixed(M, lower = "number", upper = "shade", addshade = "all")
#Correlogram
#el color azul indica correlación positiva, el color rojo indica correlación negativa.
#Cuanto mayor es la intensidad del color más cercano a 1 en el caso positivo y a 􀀀1 en el cas
#o negativo se encuentra la correlación.
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
library("corrplot")
library("readxl")
# Importa la base desde el archivo de excel
IMCInfantil = read_excel("../Data Sets Apunte Teórico/IMCinfantil.xlsx")
#Almacena la base en memoria
attach(IMCInfantil)
#Arma una sub-base con las variables seleccionadas de IMCInfantil
base.niños = data.frame(EDAD, PESO, TALLA, IMC, CC)
#Modifica la variable para que correlacione de forma negativa con las otras
base.niños$CC = max(base.niños$CC) - base.niños$CC
#Calcula la matriz de correlación
M = cor(base.niños)
#Genera el correlograma
corrplot.mixed(M, lower = "number", upper = "shade", addshade = "all")
install.packages("corrplot")
#Correlogram
#el color azul indica correlación positiva, el color rojo indica correlación negativa.
#Cuanto mayor es la intensidad del color más cercano a 1 en el caso positivo y a 􀀀1 en el cas
#o negativo se encuentra la correlación.
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
library("corrplot")
library("readxl")
# Importa la base desde el archivo de excel
IMCInfantil = read_excel("../Data Sets Apunte Teórico/IMCinfantil.xlsx")
#Almacena la base en memoria
attach(IMCInfantil)
#Arma una sub-base con las variables seleccionadas de IMCInfantil
base.niños = data.frame(EDAD, PESO, TALLA, IMC, CC)
#Modifica la variable para que correlacione de forma negativa con las otras
base.niños$CC = max(base.niños$CC) - base.niños$CC
#Calcula la matriz de correlación
M = cor(base.niños)
#Genera el correlograma
corrplot.mixed(M, lower = "number", upper = "shade", addshade = "all")
install.packages("MASS")
install.packages("lattice")
install.packages("grid")
install.packages("DMwR")
#Chernoff Faces
#Los diversos datos se transforman en caras, sirve para identificar similitudes entre individuos
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
library(MASS)#Paquete de funciones y datos
library(lattice)#Paquete para visaulizar datos
library(grid)#Paquete con un sistema para gráficos
library(DMwR)#Paquete con funciones para datamining
cov1=cov.rob(stack.x,method="mcd",nsamp="exact")#Calcula MCD
cov2=cov.rob(stack.x,method="mve",nsamp="best")#Calcula MVE
cov3=cov.rob(stack.x,method="classical",nsamp="best")
#Calcula la matriz de covarianzas clásica
center1=apply(stack.x,2,mean)#Calcula el vector de medias
center2=apply(stack.x,2,median)#Calcula el vector de medianas
dcov1=0;dcov2=0;dcov3=0#Inicializaciones
for(iin1:21){
dcov1[i]=mahalanobis(stack.x[i,],cov1$center,cov1$cov,inverted=FALSE)
dcov2[i]=mahalanobis(stack.x[i,],cov2$center,cov2$cov,inverted=FALSE)
dcov3[i]=mahalanobis(stack.x[i,],cov3$center,cov3$cov,inverted=FALSE)
}
#Calcula distancias de Mahalanobis utilizando las distintas estimaciones
#de la matriz de covarianzas
round(cbind(dcov1,dcov2,dcov3),2)
#Combina las tres distancias parao bservar el resultado
distancias.outliers=lofactor(stack.x,k=5)
#Calcula las distancias teniendo en cuenta cinco vecinos
plot(density(distancias.outliers),col="royalblue",main="",
xlab="n=21,anchodebanda=0.06518",ylab="Densidad")
#Dibuja la densidad estimada de las distancias de Mahalanobis de las
#observaciones
outliers=order(distancias.outliers,decreasing=T)[1:5]
#Arroja las observaciones correspondientes a las cinco distancias mayores
print(outliers)
#Chernoff Faces
#Los diversos datos se transforman en caras, sirve para identificar similitudes entre individuos
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
library(MASS)#Paquete de funciones y datos
library(lattice)#Paquete para visaulizar datos
library(grid)#Paquete con un sistema para gráficos
library(DMwR)#Paquete con funciones para datamining
cov1=cov.rob(stack.x,method="mcd",nsamp="exact")#Calcula MCD
cov2=cov.rob(stack.x,method="mve",nsamp="best")#Calcula MVE
cov3=cov.rob(stack.x,method="classical",nsamp="best")
#Calcula la matriz de covarianzas clásica
center1=apply(stack.x,2,mean)#Calcula el vector de medias
center2=apply(stack.x,2,median)#Calcula el vector de medianas
dcov1=0;dcov2=0;dcov3=0#Inicializaciones
for(i in 1:21){
dcov1[i]=mahalanobis(stack.x[i,],cov1$center,cov1$cov,inverted=FALSE)
dcov2[i]=mahalanobis(stack.x[i,],cov2$center,cov2$cov,inverted=FALSE)
dcov3[i]=mahalanobis(stack.x[i,],cov3$center,cov3$cov,inverted=FALSE)
}
#Calcula distancias de Mahalanobis utilizando las distintas estimaciones
#de la matriz de covarianzas
round(cbind(dcov1,dcov2,dcov3),2)
#Combina las tres distancias parao bservar el resultado
distancias.outliers=lofactor(stack.x,k=5)
#Calcula las distancias teniendo en cuenta cinco vecinos
plot(density(distancias.outliers),col="royalblue",main="",
xlab="n=21,anchodebanda=0.06518",ylab="Densidad")
#Dibuja la densidad estimada de las distancias de Mahalanobis de las
#observaciones
outliers=order(distancias.outliers,decreasing=T)[1:5]
#Arroja las observaciones correspondientes a las cinco distancias mayores
print(outliers)
A=matrix(c(1,2,􀀀1,1,0,1,3,1,0,0,2,0,0,0,1,􀀀1),nrow=4,ncol=4,byrow=T)
A=matrix(c(1,2,1,1,0,1,3,1,0,0,2,0,0,0,1,1),nrow=4,ncol=4,byrow=T)
A
eigen(A)$values
sum(diag(A))
sum(eigen(A)$values)
t(A)
sum(diag(t(A)))
det(t(A))
det(A)
eigen(t(A))$values
solve(A)
A%solve(A)
A%*%solve(A)
det(solve(A))
det(A)
eigen(solve(A))$values
eigen(A)$vectors
eigen(A)$vectors[,1]
eigen(A)$vectors[1,]
eigen(A)$vectors[,1]
A%*%eigen(A)$vectors[1,]
2*eigen(A)$vectors[1,]
A%*%eigen(A)$vectors[,1]
2*eigen(A)$vectors[,1]
A%*%eigen(A)$vectors[,1]
2*eigen(A)$vectors[,1]
A%*%eigen(A)$vectors[,1]
2*eigen(A)$vectors[,1]
n autovector de autovalor 2
A%*%eigen(A)$vectors[1,]
2*eigen(A)$vectors[1,]
A%*%eigen(A)$vectors[1,]
2*eigen(A)$vectors[1,]
#Verificamos que es un autovector de autovalor 2
A%*%eigen(A)$vectors[,1]
2*eigen(A)$vectors[,1]
sqrt(sum(eigen(A)$vectors[,1]^2))
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
#Importa Archivo para trabajar
pacientes = read_excel("../Data Sets Apunte Teórico/riesgo.xlsx")
attach(pacientes)
detach(IMCInfantil)
install.packages(ggrepel)
install.packages(ggrepel)
install.packages("ggrepel")
#Dispersograma dos variables
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
#Importa Archivo para trabajar
riesgo = read_excel("../Data Sets Apunte Teórico/riesgo.xlsx")
attach(riesgo)
install.packages("ggrepel")
#Libraría para graficar
library("ggplot2")
#Librería para manipular etiquetas de los gráficos
library("ggrepel")
#Librería para leer xlsx
library("readxl")
#Generamos el dispersograma
ggplot(riesgo, aes(x = PESO, y = SUPERFICIE)) +
geom_point(colour = "royalblue", shape = 8) +
xlab("Peso") +
ylab("Superficie_corporal") +
geom_text_repel(aes(label = rownames(riesgo), size = 2)) +
theme(legend.position = "none")
#Dispersograma dos variables
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
#Importa Archivo para trabajar
riesgo = read_excel("../Data Sets Apunte Teórico/riesgo.xlsx")
attach(riesgo)
install.packages("ggrepel")
#Libraría para graficar
library("ggplot2")
#Librería para manipular etiquetas de los gráficos
library("ggrepel")
#Librería para leer xlsx
library("readxl")
#Generamos el dispersograma
ggplot(riesgo, aes(x = PESO, y = SUPERFICIE)) +
geom_point(colour = "royalblue", shape = 8) +
xlab("Peso") +
ylab("Superficie_corporal") +
geom_text_repel(aes(label = rownames(riesgo), size = 2)) +
theme(legend.position = "none")
install.packages("ggrepel")
rm(list=ls())
setwd("E:/UBA/Análisis Inteligente de Datos/Scripts Apunte Teórico")
#Importa Archivo para trabajar
riesgo = read_excel("../Data Sets Apunte Teórico/riesgo.xlsx")
attach(riesgo)
install.packages("ggrepel")
#Libraría para graficar
library("ggplot2")
#Librería para manipular etiquetas de los gráficos
library("ggrepel")
#Librería para leer xlsx
library("readxl")
#Generamos el dispersograma
ggplot(riesgo, aes(x = PESO, y = SUPERFICIE)) +
geom_point(colour = "royalblue", shape = 8) +
xlab("Peso") +
ylab("Superficie_corporal") +
geom_text_repel(aes(label = rownames(riesgo), size = 2)) +
theme(legend.position = "none")
